# Model-Comparison-for-Dimension-Reduction

This notebook is to apply different dimension reduction algorithms to the data generated by latent predictor/sparse signal DGP and see which works best in different conditions. (I mainly take reference from the content of chapter 10 of CityU MS6218 Data Science in Marketing class which was taught by Prof. Gavin Feng)

In machine learning, we aim to compress our dataset onto a lower dimension to avoid overfitting. While there are a bunch of algorithms to serve this purpose, no single one is superior to others all the time.

The models we consider are:
  •	Shrinkage (Ridge and Lasso)
  •	Feature extraction (PCA and PLS)
  •	Feature selection (Forward stepwise and Best subset)
  •	OLS
  •	Oracle (True model)
  
Here are 2 data generating approaches:
  •	Latent predictor data generating process
  •	Sparse signal data generating process
  
In my notebook, I generated the data using “Latent” with 1,000 observations, 20 predictors and 5 true predictors. It turns out that Lasso is the best performer while feature selection approaches are worse than OLS (without any dimension reduction) if we limit the maximum number of predictor as 7.

In contract, if we generate data through “Sparse signal” with same set of parameters (you may try it out yourself), it favors feature selection models.

For further comparisons, we can also tune the number of true predictors (K), observed predictors (P) and the observations (N) and see how they affect the performance of each model.
