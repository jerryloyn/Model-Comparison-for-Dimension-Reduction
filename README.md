# Model-Comparison-for-Dimension-Reduction
Notebook for applying different dimension reduction algorithms to data generated by latent predictor/sparse signal DGP

This notebook is to summarize chapter 10 of MS6218 Data Science in Marketing in CityU which was taught by Prof. Gavin Feng.

In machine learning, we aim to compress our dataset onto a lower dimension to avoid over fitting. While there are a bunch of algorithms to serve this purpose, no single one is superior to others all the time.

The objective of this notebook is to compare a variety of models and see which works best in different conditions.

The models we consider are:
  •	Shrinkage (Ridge and Lasso)
  •	Feature extraction (PCA and PLS)
  •	Feature selection (Forward stepwise and Best subset)
  •	OLS
  •	Oracle (True model)
  
Here are 2 data generating approaches:
  •	Latent predictor data generating process
  •	Sparse signal data generating process
  
In my notebook, I generated the data using “Latent” with 1,000 observations, 20 predictors and 5 true predictors. It turns out that Lasso is the best performer (with the lowest mean squared error (MSE)) while feature selection approaches are worse than OLS (without any dimension reduction) if we limit the maximum number of predictor as 7.

In contract, if we generate data through “Sparse signal” with same set of parameters (you may try it out yourself), it favors feature selection models.

For further comparisons, we can also tune the number of true predictors (K), observed predictors (P) and the observations (N) and see how they affect the performance of each model.
